{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86895571",
   "metadata": {},
   "source": [
    "##Deep Learning\n",
    "- BNN ANN\n",
    "\n",
    "-Weight\n",
    "\n",
    "Architecture of Deep Learning:\n",
    "    - Several hidden layers with millions of artificial neaurons linked\n",
    "    - Weight represent connection between neurons\n",
    "    - Weight is +ve if node excites another , -ve if node supress another\n",
    "    - Nodes with higher values have more influence on other nodes\n",
    "    - Any input can connect to any output\n",
    "    - Need more leaning than other ML\n",
    "    - Need more training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a13212",
   "metadata": {},
   "source": [
    "# Perceptrons : Simplest ANN (Single Node)\n",
    "Components of Perceptron\n",
    "    - Input Features\n",
    "    - Weights\n",
    "    - Summation Function\n",
    "    - Activation Function\n",
    "    - Output\n",
    "    - Bias\n",
    "    - Learning Algorithm (Learning Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde4ce4f",
   "metadata": {},
   "source": [
    "## Forward Propogation -- Left to Right flow\n",
    "    - Input layer\n",
    "    - Weights and Connections\n",
    "    - Hidden Layers\n",
    "    - Output\n",
    "    - Activation Function\n",
    "        - Sigmoid Function\n",
    "            - Commnly used for binary classification\n",
    "            - Formula (1/1+e^z)\n",
    "        - Relu Function (Rectified Linear Unit)\n",
    "            - ReLU(x) = max(0,x)\n",
    "        - Softmax Function (Multiclass classification)\n",
    "        - Tanh Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c55db97",
   "metadata": {},
   "source": [
    "## BackPropogation\n",
    "    - Loss Calculation\n",
    "    - Loss Function\n",
    "    - Gradient Descent\n",
    "        - Optimization function in ML and DL for minimizing Loss function\n",
    "        - Primary goal to reduce diff between y^ and y\n",
    "        - How it works:\n",
    "            - Initialization: Start with random value for model param\n",
    "            - Compute the loss: Calc loss using loss function\n",
    "            - Calculate the Gradient: Calc the gradient of loss function and adjust params (Weight & loss)\n",
    "            - Update parameters\n",
    "            - Iterate\n",
    "    - Adjusted weights\n",
    "    - Training\n",
    "    - Activation Function\n",
    "    - Epoch = Forward + BackPropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da342a70",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
